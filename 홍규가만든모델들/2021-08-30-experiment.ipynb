{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## 0. Libarary ë¶ˆëŸ¬ì˜¤ê¸° ë° ê²½ë¡œì„¤ì •"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import os\r\n",
    "import pandas as pd\r\n",
    "from PIL import Image\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "\r\n",
    "import torch\r\n",
    "import torch.nn as nn\r\n",
    "import torch.nn.functional as F\r\n",
    "from torch.utils.data import Dataset, DataLoader\r\n",
    "from glob import glob\r\n",
    "import torchvision\r\n",
    "from torchvision import transforms\r\n",
    "from torchvision.transforms import Resize, ToTensor, Normalize\r\n",
    "\r\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "# A = torch.tensor([[100,1000,1],[1,1,10]])\r\n",
    "# b = torch.argmax(A, dim=1) \r\n",
    "# c = torch.tensor([0,2])\r\n",
    "# torch.tensor(b==c)\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/tmp/ipykernel_40057/2434658839.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(b==c)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([False,  True])"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## testê´€ë ¨ ì„¸íŒ…\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "# í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ í´ë” ê²½ë¡œë¥¼ ì§€ì •í•´ì£¼ì„¸ìš”.\r\n",
    "test_dir = os.path.join(os.getcwd() , 'datasets/eval/')\r\n",
    "submission = pd.read_csv(os.path.join(test_dir, 'info.csv'))\r\n",
    "image_dir = os.path.join(test_dir, 'images')\r\n",
    "\r\n",
    "# Test Dataset í´ë˜ìŠ¤ ê°ì²´ë¥¼ ìƒì„±í•˜ê³  DataLoaderë¥¼ ë§Œë“­ë‹ˆë‹¤.\r\n",
    "image_paths = [os.path.join(image_dir, img_id) for img_id in submission.ImageID] #imgDirList\r\n",
    "# ì´ë¯¸ì§€ ì£¼ì†Œ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•´ì¤Œ\r\n",
    "\r\n",
    "transforms_test = transforms.Compose([\r\n",
    "    transforms.Resize((224, 224)),\r\n",
    "    #transforms.RandomHorizontalFlip(), #Data Augmentation ë°ì´í„° ì¦ì§„?!\r\n",
    "    transforms.ToTensor(),\r\n",
    "    transforms.Normalize(\r\n",
    "        [0.485, 0.456, 0.406],\r\n",
    "        [0.229, 0.224, 0.225],\r\n",
    "    ),\r\n",
    "])\r\n",
    "\r\n",
    "\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "transforms_train = transforms.Compose([\r\n",
    "    transforms.Resize((224, 224)),\r\n",
    "    transforms.RandomHorizontalFlip(), #Data Augmentation ë°ì´í„° ì¦ì§„?!\r\n",
    "    transforms.ToTensor(),\r\n",
    "    transforms.Normalize(\r\n",
    "        [0.485, 0.456, 0.406],\r\n",
    "        [0.229, 0.224, 0.225],\r\n",
    "    ),\r\n",
    "])\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Model ì •ì˜"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "\r\n",
    "\r\n",
    "gender_classifier = torchvision.models.resnet34(pretrained=True)\r\n",
    "gender_classifier.fc = torch.nn.Sequential(\r\n",
    "    nn.Linear(in_features=512, out_features=2, bias=True),\r\n",
    "    nn.Sigmoid(),\r\n",
    ")#ageëŠ” ì„ í˜•íšŒê·€ë¡œ ë§Œë“¤ì–´ë³´ê¸°\r\n",
    "\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h1 style=\"color='blue'\">Train Dataset ì •ì˜</h1>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "class Train_Datasets (torch.utils.data.Dataset):\r\n",
    "    _train_csv = pd.read_csv('./datasets/train/train.csv')\r\n",
    "    dir_list=  glob('./datasets/train/images/*/*')\r\n",
    "    \r\n",
    "\r\n",
    "    def __len__ (self):\r\n",
    "        return self.dir_list.__len__()\r\n",
    "    \r\n",
    "    def __getitem__ (self , idx):\r\n",
    "        dir = self.dir_list[idx],\r\n",
    "        _wear = ''\r\n",
    "        if self.dir_list[idx].split('/')[-1].startswith('mask') :\r\n",
    "            _wear = 2\r\n",
    "        elif self.dir_list[idx].split('/')[-1].startswith('inc'):\r\n",
    "            _wear = 1\r\n",
    "        else : \r\n",
    "            _wear = 0\r\n",
    "        id, gender, _ , age = self.dir_list[idx].split('/')[-2].split('_')\r\n",
    "        gender = 0 if gender == 'female' else 1\r\n",
    "        img = Image.open(self.dir_list[idx]) #í•´ë‹¹ ì£¼ì†Œë¡œ ì´ë¯¸ì§€ ì—´ê³ \r\n",
    "        X = transforms_train(img)\r\n",
    "        age = int(age)\r\n",
    "        if age <30 :\r\n",
    "            age = 0 \r\n",
    "        elif age >= 30 and age <60 :\r\n",
    "            age =1\r\n",
    "        else:\r\n",
    "            age=2\r\n",
    "\r\n",
    "        #.unsqueeze(0)  if transforms_train(img).shape.__len__() == 3 else  transforms_train(img)# íŠ¸ëœìŠ¤í¼ì— ì „ë‹¬í•´ì„œ í…ì„œí™”\r\n",
    "        y = (\r\n",
    "            torch.tensor(gender), \r\n",
    "            torch.tensor(_wear), \r\n",
    "            torch.tensor(age)\r\n",
    "        ) #yëŠ” ìš°ë¦¬ê°€ ëª¨ë¸ë¡œ íŒë³„í•˜ê³ ì í•˜ëŠ” ìš”ì†Œë“¤, ì  ë”, ë§ˆìŠ¤í¬ì°©ìš©í˜•íƒœ, age\r\n",
    "        return X, y\r\n",
    "\r\n",
    "# yëŠ” gender , ë§ˆìŠ¤í¬ ì°©ìš©ì—¬ë¶€, ë‚˜ì´ \r\n",
    "# genderëŠ” ì—¬ìê°€ 0, ë‚¨ìê°€ 1\r\n",
    "# ë§ˆìŠ¤í¬ëŠ” ì œëŒ€ë¡œ ì“°ë©´ 2, ì´ìƒí•˜ê²Œ ì“°ë©´ 1, ì•ˆì“°ë©´ ,0\r\n",
    "        "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "train_datasets = Train_Datasets()\r\n",
    "train_databatch = torch.utils.data.DataLoader(\r\n",
    "    train_datasets,\r\n",
    "    batch_size = 100,\r\n",
    "    shuffle = True,\r\n",
    "    num_workers = 4,\r\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 1. ë§ˆìŠ¤í¬ ë¶„ë¥˜ëª¨ë¸ ëª¨ë¸\n",
    "### ë¼ë²¨ë§\n",
    "- <mark>[ë¯¸ì°©ìš©, ì˜¤ì°©ìš©, ì •ìƒì°©ìš©]</mark> ìˆœì„œë¡œ ë¼ë²¨ë§ ë˜ì–´ ìˆìŒ."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "# 1. ëª¨ë¸ì˜ ì •ì˜\r\n",
    "# ì „ì´í•™ìŠµ ëª¨ë¸ë¡œ ë§Œë“¦\r\n",
    "mask_classifier = torchvision.models.resnet34(pretrained=True)\r\n",
    "mask_classifier.fc = nn.Sequential(\r\n",
    "    nn.Linear(in_features = 512, out_features = 3, bias=True),\r\n",
    "    nn.Softmax(),\r\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "# ğŸŒŠí…ì„œë³´ë“œ ê´€ë ¨\r\n",
    "writer = SummaryWriter()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# 2. í•™ìŠµëª¨ë“œ\r\n",
    "epochs = 1\r\n",
    "def mask_criterion (pred, y):\r\n",
    "    #ë§ˆìŠ¤í¬ëŠ” 3ê°€ì§€ í´ë˜ìŠ¤ë¡œ ë¶„ë¥˜.\r\n",
    "    #ì¦‰ ì‹¤ì œ ì •ë‹µ yëŠ” 0,1,2 ì…‹ ì¤‘ í•˜ë‚˜ë¥¼ ê°’ìœ¼ë¡œ ê°€ì§.\r\n",
    "    pred = -1 * torch.log2(pred) # ëª¨ë¸ì´ ì˜ˆì¸¡í•œ í´ë˜ìŠ¤ë³„ í™•ë¥ (=pred) ì „ì²´ì— ë¡œê·¸ë¥¼ ì”Œì›€.... \r\n",
    "    pred = 0.025 * pred # 0.025ë¥¼ ì¼ë‹¨ ê³±í•´ì£¼ì. ì™œëƒí•˜ë©´ ì¼ì¢…ì˜ ë¼ë²¨ ìŠ¤ë¬´ë”©ì„...\r\n",
    "    # ë¼ë²¨ì— í•´ë‹¹ë˜ëŠ” ì›ì†ŒëŠ” 0.95/0.025=38ë¥¼ ê³±í•´ì£¼ì.\r\n",
    "    for idx, label in enumerate(y): #yë¡œ ë°˜ë³µë¬¸(for)ì„ ëŒë¦¬ì.\r\n",
    "        pred[idx,label ] = 38 * pred[idx,label ]\r\n",
    "    \r\n",
    "    return torch.sum(pred)\r\n",
    "    \r\n",
    "mask_opti =torch.optim.Adam(mask_classifier.parameters(), lr=0.001)\r\n",
    "device = torch.device('cuda')\r\n",
    "mask_classifier.to(device).train()\r\n",
    "\r\n",
    "for _i_d_x__ in range(epochs):\r\n",
    "    for i, ( X, y ) in enumerate(train_databatch):\r\n",
    "        X = X.to(device)\r\n",
    "        _ ,mask_real , __ = y\r\n",
    "        mask_real = mask_real.to(device)\r\n",
    "        print(mask_real)\r\n",
    "        mask_opti.zero_grad()\r\n",
    "        mask_pred = mask_classifier(X)     \r\n",
    "        print('ëª¨ë¸ ì˜ˆì¸¡ ê°’ ', mask_pred)\r\n",
    "        print('-'*50)\r\n",
    "        loss_mask = mask_criterion(mask_pred, mask_real)\r\n",
    "        print( i, 'mask _loss : ', loss_mask)\r\n",
    "        writer.add_scalar(\r\n",
    "            \"Loss/train\", \r\n",
    "            loss_mask, \r\n",
    "            i + train_datasets.__len__()*_i_d_x__\r\n",
    "        )\r\n",
    "        loss_mask.backward()\r\n",
    "        mask_opti.step()\r\n",
    "writer.flush()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "!tensorboard --logdir=runs\r\n",
    "#writer.close()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Test ëª¨ë“œ\r\n",
    "test_model = mask_classifier \r\n",
    "imglist = glob('./datasets/eval/images/*')\r\n",
    "transforms_test = transforms.Compose([\r\n",
    "    transforms.Resize((224, 224)),\r\n",
    "    transforms.ToTensor(),\r\n",
    "    transforms.Normalize(\r\n",
    "        [0.485, 0.456, 0.406],\r\n",
    "        [0.229, 0.224, 0.225],\r\n",
    "    ),\r\n",
    "])\r\n",
    "test_model = test_model.to(device)\r\n",
    "test_model.eval()\r\n",
    "with torch.no_grad() :\r\n",
    "    for idx, dir in enumerate(imglist):\r\n",
    "        print(dir)\r\n",
    "        if idx < 10 : \r\n",
    "            continue\r\n",
    "        if idx == 20 :\r\n",
    "            break\r\n",
    "        img = Image.open(dir)\r\n",
    "        plt.imshow(img)\r\n",
    "        img = transforms_test(img).to(torch.device('cuda'))\r\n",
    "        age_predict = test_model(img.unsqueeze(0))\r\n",
    "        plt.title(age_predict)\r\n",
    "        plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2. ì—°ë ¹ ë¶„ë¥˜ ëª¨ë¸"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "source": [
    "age_classifier = torchvision.models.resnet34(pretrained=True)\r\n",
    "\r\n",
    "age_classifier.fc = nn.Sequential(\r\n",
    "    nn.Linear(in_features = 512, out_features = 3, bias=True),\r\n",
    "    nn.Softmax(),\r\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# 2. í•™ìŠµëª¨ë“œ\r\n",
    "epochs = 1\r\n",
    "def age_criterion (pred, y):\r\n",
    "    #ì—ì´ì§€ ì—­ì‹œ 3ê°€ì§€ í´ë˜ìŠ¤ë¡œ ë¶„ë¥˜.\r\n",
    "    #ì¦‰ ì‹¤ì œ ì •ë‹µ yëŠ” 0,1,2 ì…‹ ì¤‘ í•˜ë‚˜ë¥¼ ê°’ìœ¼ë¡œ ê°€ì§.\r\n",
    "    pred = -1 * torch.log2(pred) # ëª¨ë¸ì´ ì˜ˆì¸¡í•œ í´ë˜ìŠ¤ë³„ í™•ë¥ (=pred) ì „ì²´ì— ë¡œê·¸ë¥¼ ì”Œì›€.... \r\n",
    "    pred = 0.05 * pred # 0.05ë¥¼ ì¼ë‹¨ ê³±í•´ì£¼ì. ì™œëƒí•˜ë©´ ì¼ì¢…ì˜ ë¼ë²¨ ìŠ¤ë¬´ë”©ì„...\r\n",
    "    # ë¼ë²¨ì— í•´ë‹¹ë˜ëŠ” ì›ì†ŒëŠ” 0.95/0.05=19ë¥¼ ê³±í•´ì£¼ì.\r\n",
    "    for idx, label in enumerate(y): #yë¡œ ë°˜ë³µë¬¸(for)ì„ ëŒë¦¬ì.\r\n",
    "        pred[idx,label ] = 19 * pred[idx,label ]\r\n",
    "    \r\n",
    "    return torch.sum(pred)\r\n",
    "    \r\n",
    "age_opti =torch.optim.Adam(age_classifier.parameters(), lr=0.001)\r\n",
    "device = torch.device('cuda')\r\n",
    "age_classifier.to(device).train()\r\n",
    "\r\n",
    "correct_count = 0\r\n",
    "for _ in range(epochs):\r\n",
    "    \r\n",
    "    for i, (X, y) in enumerate(train_databatch):\r\n",
    "        X = X.to(device)\r\n",
    "        _ ,__ , age_real = y\r\n",
    "        age_real = age_real.to(device)\r\n",
    "        \r\n",
    "        # print(age_real)\r\n",
    "        \r\n",
    "        age_opti.zero_grad()\r\n",
    "        age_pred = age_classifier(X)   \r\n",
    "\r\n",
    "        how_much = torch.sum(torch.argmax(age_pred, dim=1) == age_real).item()\r\n",
    "        print(\r\n",
    "            i, \r\n",
    "            \"ë²ˆì§¸ ë°°ì¹˜ì—ì„œì˜ ì •ë‹µë¥ \",\r\n",
    "            how_much,\r\n",
    "            \"%\",\r\n",
    "        )\r\n",
    "        correct_count += how_much\r\n",
    "\r\n",
    "        # print('ëª¨ë¸ ì˜ˆì¸¡ ê°’ ', age_pred)\r\n",
    "        # print('-'*50)\r\n",
    "        loss_age = age_criterion(age_pred, age_real)\r\n",
    "        # print( i, 'mask _loss : ', loss_age)\r\n",
    "        loss_age.backward()\r\n",
    "        age_opti.step()\r\n",
    "print('ëª¨ë¸ ì •í™•ë„ : ', correct_count/train_datasets.__len__())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Test ëª¨ë“œ\r\n",
    "test_model = age_classifier \r\n",
    "imglist = glob('./datasets/eval/images/*')\r\n",
    "transforms_test = transforms.Compose([\r\n",
    "    transforms.Resize((224, 224)),\r\n",
    "    transforms.ToTensor(),\r\n",
    "    transforms.Normalize(\r\n",
    "        [0.485, 0.456, 0.406],\r\n",
    "        [0.229, 0.224, 0.225],\r\n",
    "    ),\r\n",
    "])\r\n",
    "test_model = test_model.to(device)\r\n",
    "test_model.eval()\r\n",
    "with torch.no_grad() :\r\n",
    "    for idx, dir in enumerate(imglist):\r\n",
    "        print(dir)\r\n",
    "        if idx < 10 : \r\n",
    "            continue\r\n",
    "        if idx == 20 :\r\n",
    "            break\r\n",
    "        img = Image.open(dir)\r\n",
    "        plt.imshow(img)\r\n",
    "        img = transforms_test(img).to(torch.device('cuda'))\r\n",
    "        age_predict = test_model(img.unsqueeze(0))\r\n",
    "        plt.title(age_predict)\r\n",
    "        plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3. ì  ë” ë¶„ë¥˜ ëª¨ë¸"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "gender_classifier = torchvision.models.resnet34(pretrained=True)\r\n",
    "\r\n",
    "gender_classifier.fc = nn.Sequential(\r\n",
    "    nn.Linear(in_features = 512, out_features = 2, bias=True),\r\n",
    "    nn.Softmax(),\r\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# 2. í•™ìŠµëª¨ë“œ\r\n",
    "epochs = 1\r\n",
    "def gender_criterion (pred, y):\r\n",
    "    #ì—ì´ì§€ ì—­ì‹œ 3ê°€ì§€ í´ë˜ìŠ¤ë¡œ ë¶„ë¥˜.\r\n",
    "    #ì¦‰ ì‹¤ì œ ì •ë‹µ yëŠ” 0,1,2 ì…‹ ì¤‘ í•˜ë‚˜ë¥¼ ê°’ìœ¼ë¡œ ê°€ì§.\r\n",
    "    pred = -1 * torch.log2(pred) # ëª¨ë¸ì´ ì˜ˆì¸¡í•œ í´ë˜ìŠ¤ë³„ í™•ë¥ (=pred) ì „ì²´ì— ë¡œê·¸ë¥¼ ì”Œì›€.... \r\n",
    "    pred = 0.025 * pred # 0.025ë¥¼ ì¼ë‹¨ ê³±í•´ì£¼ì. ì™œëƒí•˜ë©´ ì¼ì¢…ì˜ ë¼ë²¨ ìŠ¤ë¬´ë”©ì„...\r\n",
    "    # ë¼ë²¨ì— í•´ë‹¹ë˜ëŠ” ì›ì†ŒëŠ” 0.975/0.025=39ë¥¼ ê³±í•´ì£¼ì.\r\n",
    "    for idx, label in enumerate(y): #yë¡œ ë°˜ë³µë¬¸(for)ì„ ëŒë¦¬ì.\r\n",
    "        pred[idx,label ] = 39 * pred[idx,label ]\r\n",
    "    \r\n",
    "    return torch.sum(pred)\r\n",
    "    \r\n",
    "gender_opti =torch.optim.Adam(gender_classifier.parameters(), lr=0.001)\r\n",
    "device = torch.device('cuda')\r\n",
    "gender_classifier.to(device).train()\r\n",
    "\r\n",
    "\r\n",
    "correct_count = 0\r\n",
    "\r\n",
    "for _ in range(epochs):\r\n",
    "    for i,( X, y ) in enumerate( train_databatch):\r\n",
    "        X = X.to(device)\r\n",
    "        gender_real ,__ , _ = y\r\n",
    "        \r\n",
    "        gender_real = gender_real.to(device)\r\n",
    "        # print(\"ì‹¤ì œ ì„±ë³„ : \" ,gender_real)\r\n",
    "        gender_opti.zero_grad()\r\n",
    "        gender_pred = gender_classifier(X)    \r\n",
    "        \r\n",
    "\r\n",
    "        how_much = torch.sum( torch.argmax(gender_pred, dim=1) == gender_real).item()\r\n",
    "        print(\r\n",
    "            i, \r\n",
    "            \"ë²ˆì§¸ ë°°ì¹˜ì—ì„œì˜ ì •ë‹µë¥ \", \r\n",
    "            how_much,\r\n",
    "            '%',\r\n",
    "        )\r\n",
    "\r\n",
    "        correct_count += how_much\r\n",
    "\r\n",
    "        # print('ëª¨ë¸ ì˜ˆì¸¡ ê°’ ', gender_pred)\r\n",
    "        # print('-'*50)\r\n",
    "        loss_gender = gender_criterion(gender_pred, gender_real)\r\n",
    "        print('ë°°ì¹˜: ', i, '// gender _loss : ', loss_gender.item())\r\n",
    "        loss_gender.backward()\r\n",
    "        gender_opti.step()\r\n",
    "\r\n",
    "print(\r\n",
    "    \"ëª¨ë¸ ì •í™•ë„ :\", \r\n",
    "    correct_count/train_datasets.__len__())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Test ëª¨ë“œ\r\n",
    "test_model = gender_classifier \r\n",
    "imglist = glob('./datasets/eval/images/*')\r\n",
    "transforms_test = transforms.Compose([\r\n",
    "    transforms.Resize((224, 224)),\r\n",
    "    transforms.ToTensor(),\r\n",
    "    transforms.Normalize(\r\n",
    "        [0.485, 0.456, 0.406],\r\n",
    "        [0.229, 0.224, 0.225],\r\n",
    "    ),\r\n",
    "])\r\n",
    "test_model = test_model.to(device)\r\n",
    "test_model.eval()\r\n",
    "with torch.no_grad() :\r\n",
    "    for idx, dir in enumerate(imglist):\r\n",
    "        print(dir)\r\n",
    "        if idx < 10 : \r\n",
    "            continue\r\n",
    "        if idx == 20 :\r\n",
    "            break\r\n",
    "        img = Image.open(dir)\r\n",
    "        plt.imshow(img)\r\n",
    "        img = transforms_test(img).to(torch.device('cuda'))\r\n",
    "        age_predict = test_model(img.unsqueeze(0))\r\n",
    "        plt.title(age_predict)\r\n",
    "        plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. Inference"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "device = torch.device('cuda')\r\n",
    "\r\n",
    "mask_classifier.eval()\r\n",
    "gender_classifier.eval()\r\n",
    "age_classifier.eval()\r\n",
    "\r\n",
    "all_predictions =[]\r\n",
    "# ëª¨ë¸ì´ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ì„ ì˜ˆì¸¡í•˜ê³  ê²°ê³¼ë¥¼ ì €ì¥í•©ë‹ˆë‹¤.\r\n",
    "for img_dir in image_paths:\r\n",
    "    img_obj = Image.open(img_dir)\r\n",
    "    img_obj = transforms_test(img_obj).unsqueeze(0).to(device)\r\n",
    "    with torch.no_grad():\r\n",
    "        img_obj = img_obj.to(device)\r\n",
    "        pred = model(img_obj)\r\n",
    "        pred = pred.argmax(dim=-1)\r\n",
    "        pred = pred.cpu().numpy()\r\n",
    "        idx_list = [0,1,10,11,12,13,14,15,16,17,2,3,4,5,6,7,8,9]\r\n",
    "        pred = idx_list[int(pred)]\r\n",
    "        all_predictions.append(pred)\r\n",
    "submission['ans'] = all_predictions\r\n",
    "\r\n",
    "# ì œì¶œí•  íŒŒì¼ì„ ì €ì¥í•©ë‹ˆë‹¤.\r\n",
    "submission.to_csv(os.path.join(test_dir, 'submission.csv'), index=False)\r\n",
    "print('test inference is done!')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "test inference is done!\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "interpreter": {
   "hash": "98b0a9b7b4eaaa670588a142fd0a9b87eaafe866f1db4228be72b4211d12040f"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}